\subsection{AI Model Selection and Deployment} 

  

\textbf{Model Selection:}   

The HOSE Rental application will utilize an open-source Large Language Model (LLM) from the LLaMA (Large Language Model Meta AI) family to power the AI assistant functionality. 

  

\textbf{Rationale:}   

The LLaMA model was selected due to its open-source availability, zero per-request cost, and suitability for lightweight natural-language tasks such as answering user questions, summarizing rental information, and explaining property attributes. This approach allows the system to operate without reliance on third-party paid APIs and ensures full control over model deployment and data usage. 

  

\textbf{Deployment Environment:}   

The LLaMA model will be hosted locally on project-owned computing resources. The model will run as a backend service accessible by the HOSE Rental application through a local API interface. 

  

\textbf{Capabilities:} 

\begin{itemize} 

\item Interpret natural-language user queries related to rental housing. 

\item Generate informational responses based on data supplied by the application. 

\item Summarize user reviews and rental property details. 

\item Assist users in comparing rental options using descriptive language. 

\end{itemize} 

  

\textbf{Limitations:} 

\begin{itemize} 

\item The AI assistant will not provide legal, financial, or contractual advice. 

\item The model will not independently calculate distances or access real-time external data. 

\item All factual information used by the model must be supplied by the application database or supporting APIs. 

\end{itemize} 

  

\textbf{Data Privacy and Control:}   

Because the LLaMA model is hosted locally, no user queries or system data are transmitted to external AI service providers. This ensures improved privacy, reduced latency, and compliance with academic project constraints. 

  

\textbf{Future Scalability:}   

The system architecture allows for future replacement or augmentation of the LLaMA model with alternative LLMs or cloud-based services if scalability or performance requirements change. 

 

 

 

\section{Non-Functional Requirements} 

  

\subsection{Hardware Requirements} 

  

\textbf{Local LLM Hosting:}   

The HOSE Rental application shall support execution of a locally hosted LLaMA-based Large Language Model (LLM) on project-owned computing hardware without reliance on external cloud resources. 

  

\textbf{Minimum Hardware Requirements:} 

\begin{itemize} 

\item CPU: Quad-core 64-bit processor or equivalent 

\item System Memory (RAM): Minimum 16 GB 

\item Storage: Minimum 20 GB of available disk space for model files and application data 

\item Operating System: Windows, macOS, or Linux with support for local backend services 

\end{itemize} 

  

\textbf{Recommended Hardware Requirements:} 

\begin{itemize} 

\item CPU: Multi-core processor (8 cores or higher recommended) 

\item System Memory (RAM): 32 GB or more 

\item GPU (Optional): Dedicated GPU with a minimum of 8 GB VRAM to improve response latency 

\end{itemize} 

  

\textbf{Justification:}   

The selected LLaMA model variant is intended for lightweight natural-language processing tasks, such as informational question answering and summarization. These tasks do not require large-scale computational resources or model training. The listed hardware requirements are achievable using standard consumer-grade or academic lab computers and are sufficient to support acceptable response times for a limited number of concurrent users. 

  

\textbf{Scalability Considerations:}   

The system is designed for small-scale academic usage. Increased user demand may require additional hardware resources or migration to a cloud-based LLM service in future iterations. 